function [output, state] = importedLungmaskFcn_R231(input, params, varargin)
%IMPORTEDLUNGMASKFCN_R231 Function implementing an imported ONNX network.
%
% THIS FILE WAS AUTO-GENERATED BY importONNXFunction.
% ONNX Operator Set Version: 10
%
% Variable names in this function are taken from the original ONNX file.
%
% [OUTPUT] = importedLungmaskFcn_R231(INPUT, PARAMS)
%			- Evaluates the imported ONNX network IMPORTEDLUNGMASKFCN_R231 with input(s)
%			INPUT and the imported network parameters in PARAMS. Returns
%			network output(s) in OUTPUT.
%
% [OUTPUT, STATE] = importedLungmaskFcn_R231(INPUT, PARAMS)
%			- Additionally returns state variables in STATE. When training,
%			use this form and set TRAINING to true.
%
% [__] = importedLungmaskFcn_R231(INPUT, PARAMS, 'NAME1', VAL1, 'NAME2', VAL2, ...)
%			- Specifies additional name-value pairs described below:
%
% 'Training'
% 			Boolean indicating whether the network is being evaluated for
%			prediction or training. If TRAINING is true, state variables
%			will be updated.
%
% 'InputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			 between the dimensions of the input data and the dimensions of
%			the ONNX model input. For example, the permutation from HWCN
%			(MATLAB standard) to NCHW (ONNX standard) uses the vector
%			[4 3 1 2]. See the documentation for IMPORTONNXFUNCTION for
%			more information about automatic permutation.
%
%			'none' - Input(s) are passed in the ONNX model format. See 'Inputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between input data dimensions and the expected
%			ONNX input dimensions.%
%			cell array - If the network has multiple inputs, each cell
%			contains 'auto', 'none', or a numeric vector.
%
% 'OutputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			between the dimensions of the output and a conventional MATLAB
%			dimension ordering. For example, the permutation from NC (ONNX
%			standard) to CN (MATLAB standard) uses the vector [2 1]. See
%			the documentation for IMPORTONNXFUNCTION for more information
%			about automatic permutation.
%
%			'none' - Return output(s) as given by the ONNX model. See 'Outputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between the ONNX output dimensions and the
%			desired output dimensions.%
%			cell array - If the network has multiple outputs, each cell
%			contains 'auto', 'none' or a numeric vector.
%
% Inputs:
% -------
% INPUT
%			- Input(s) to the ONNX network.
%			  The input size(s) expected by the ONNX file are:
%				  INPUT:		[batch_size, 1, 256, 256]				Type: FLOAT
%			  By default, the function will try to permute the input(s)
%			  into this dimension ordering. If the default is incorrect,
%			  use the 'InputDataPermutation' argument to control the
%			  permutation.
%
%
% PARAMS	- Network parameters returned by 'importONNXFunction'.
%
%
% Outputs:
% --------
% OUTPUT
%			- Output(s) of the ONNX network.
%			  Without permutation, the size(s) of the outputs are:
%				  OUTPUT:		[batch_size, 3, 256, 256]				Type: FLOAT
%			  By default, the function will try to permute the output(s)
%			  from this dimension ordering into a conventional MATLAB
%			  ordering. If the default is incorrect, use the
%			  'OutputDataPermutation' argument to control the permutation.
%
% STATE		- (Optional) State variables. When TRAINING is true, these will
% 			  have been updated from the original values in PARAMS.State.
%
%
%  See also importONNXFunction

% Preprocess the input data and arguments:
[input, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(input, params, varargin{:});
% Put all variables into a single struct to implement dynamic scoping:
[Vars, NumDims] = packageVariables(params, {'input'}, {input}, [4]);
% Call the top-level graph function:
[output, outputNumDims, state] = torch_jit_exportGraph1000(input, NumDims.input, Vars, NumDims, Training, params.State);
% Postprocess the output data
[output] = postprocessOutput(output, outputDataPerms, anyDlarrayInputs, Training, varargin{:});
end

function [output, outputNumDims1105, state] = torch_jit_exportGraph1000(input, inputNumDims1104, Vars, NumDims, Training, state)
% Function implementing the graph 'torch_jit_exportGraph1000'
% Update Vars and NumDims from the graph's formal input parameters. Note that state variables are already in Vars.
Vars.input = input;
NumDims.input = inputNumDims1104;

% Execute the operators:
% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x228] = prepareConvArgs(Vars.down_path_0_block_0_weight, Vars.down_path_0_block_0_bias, Vars.ConvStride1001, Vars.ConvDilationFactor1002, Vars.ConvPadding1003, 1, NumDims.input, NumDims.down_path_0_block_0_weight);
Vars.x228 = dlconv(Vars.input, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x229 = relu(Vars.x228);
NumDims.x229 = NumDims.x228;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x230, NumDims.down_path_0_block_2_running_mean, NumDims.down_path_0_block_2_running_var] = prepareBatchNormalizationArgs(Vars.down_path_0_block_2_bias, Vars.down_path_0_block_2_weight, Vars.down_path_0_block_2_running_mean, Vars.down_path_0_block_2_running_var, NumDims.x229, NumDims.down_path_0_block_2_running_mean, NumDims.down_path_0_block_2_running_var);
if Training
    [Vars.x230, dsmean, dsvar] = batchnorm(Vars.x229, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_0_block_2_running_mean = dlarray(dsmean);
    Vars.down_path_0_block_2_running_var = dlarray(dsvar);
else
    Vars.x230 = batchnorm(Vars.x229, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_0_block_2_running_mean = Vars.down_path_0_block_2_running_mean;
state.down_path_0_block_2_running_var = Vars.down_path_0_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x231] = prepareConvArgs(Vars.down_path_0_block_3_weight, Vars.down_path_0_block_3_bias, Vars.ConvStride1004, Vars.ConvDilationFactor1005, Vars.ConvPadding1006, 1, NumDims.x230, NumDims.down_path_0_block_3_weight);
Vars.x231 = dlconv(Vars.x230, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x232 = relu(Vars.x231);
NumDims.x232 = NumDims.x231;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x233, NumDims.down_path_0_block_5_running_mean, NumDims.down_path_0_block_5_running_var] = prepareBatchNormalizationArgs(Vars.down_path_0_block_5_bias, Vars.down_path_0_block_5_weight, Vars.down_path_0_block_5_running_mean, Vars.down_path_0_block_5_running_var, NumDims.x232, NumDims.down_path_0_block_5_running_mean, NumDims.down_path_0_block_5_running_var);
if Training
    [Vars.x233, dsmean, dsvar] = batchnorm(Vars.x232, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_0_block_5_running_mean = dlarray(dsmean);
    Vars.down_path_0_block_5_running_var = dlarray(dsvar);
else
    Vars.x233 = batchnorm(Vars.x232, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_0_block_5_running_mean = Vars.down_path_0_block_5_running_mean;
state.down_path_0_block_5_running_var = Vars.down_path_0_block_5_running_var;

% Pad:
[Vars.x234, NumDims.x234] = onnxPad(Vars.x233, Vars.PadPadding1007, 0.000000, 'constant', NumDims.x233);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.x235] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1008, Vars.AveragePoolStride1009, Vars.AveragePoolPadding1010, 0, NumDims.x234);
Vars.x235 = avgpool(Vars.x234, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x236] = prepareConvArgs(Vars.down_path_1_block_0_weight, Vars.down_path_1_block_0_bias, Vars.ConvStride1011, Vars.ConvDilationFactor1012, Vars.ConvPadding1013, 1, NumDims.x235, NumDims.down_path_1_block_0_weight);
Vars.x236 = dlconv(Vars.x235, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x237 = relu(Vars.x236);
NumDims.x237 = NumDims.x236;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x238, NumDims.down_path_1_block_2_running_mean, NumDims.down_path_1_block_2_running_var] = prepareBatchNormalizationArgs(Vars.down_path_1_block_2_bias, Vars.down_path_1_block_2_weight, Vars.down_path_1_block_2_running_mean, Vars.down_path_1_block_2_running_var, NumDims.x237, NumDims.down_path_1_block_2_running_mean, NumDims.down_path_1_block_2_running_var);
if Training
    [Vars.x238, dsmean, dsvar] = batchnorm(Vars.x237, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_1_block_2_running_mean = dlarray(dsmean);
    Vars.down_path_1_block_2_running_var = dlarray(dsvar);
else
    Vars.x238 = batchnorm(Vars.x237, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_1_block_2_running_mean = Vars.down_path_1_block_2_running_mean;
state.down_path_1_block_2_running_var = Vars.down_path_1_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x239] = prepareConvArgs(Vars.down_path_1_block_3_weight, Vars.down_path_1_block_3_bias, Vars.ConvStride1014, Vars.ConvDilationFactor1015, Vars.ConvPadding1016, 1, NumDims.x238, NumDims.down_path_1_block_3_weight);
Vars.x239 = dlconv(Vars.x238, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x240 = relu(Vars.x239);
NumDims.x240 = NumDims.x239;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x241, NumDims.down_path_1_block_5_running_mean, NumDims.down_path_1_block_5_running_var] = prepareBatchNormalizationArgs(Vars.down_path_1_block_5_bias, Vars.down_path_1_block_5_weight, Vars.down_path_1_block_5_running_mean, Vars.down_path_1_block_5_running_var, NumDims.x240, NumDims.down_path_1_block_5_running_mean, NumDims.down_path_1_block_5_running_var);
if Training
    [Vars.x241, dsmean, dsvar] = batchnorm(Vars.x240, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_1_block_5_running_mean = dlarray(dsmean);
    Vars.down_path_1_block_5_running_var = dlarray(dsvar);
else
    Vars.x241 = batchnorm(Vars.x240, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_1_block_5_running_mean = Vars.down_path_1_block_5_running_mean;
state.down_path_1_block_5_running_var = Vars.down_path_1_block_5_running_var;

% Pad:
[Vars.x242, NumDims.x242] = onnxPad(Vars.x241, Vars.PadPadding1017, 0.000000, 'constant', NumDims.x241);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.x243] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1018, Vars.AveragePoolStride1019, Vars.AveragePoolPadding1020, 0, NumDims.x242);
Vars.x243 = avgpool(Vars.x242, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x244] = prepareConvArgs(Vars.down_path_2_block_0_weight, Vars.down_path_2_block_0_bias, Vars.ConvStride1021, Vars.ConvDilationFactor1022, Vars.ConvPadding1023, 1, NumDims.x243, NumDims.down_path_2_block_0_weight);
Vars.x244 = dlconv(Vars.x243, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x245 = relu(Vars.x244);
NumDims.x245 = NumDims.x244;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x246, NumDims.down_path_2_block_2_running_mean, NumDims.down_path_2_block_2_running_var] = prepareBatchNormalizationArgs(Vars.down_path_2_block_2_bias, Vars.down_path_2_block_2_weight, Vars.down_path_2_block_2_running_mean, Vars.down_path_2_block_2_running_var, NumDims.x245, NumDims.down_path_2_block_2_running_mean, NumDims.down_path_2_block_2_running_var);
if Training
    [Vars.x246, dsmean, dsvar] = batchnorm(Vars.x245, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_2_block_2_running_mean = dlarray(dsmean);
    Vars.down_path_2_block_2_running_var = dlarray(dsvar);
else
    Vars.x246 = batchnorm(Vars.x245, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_2_block_2_running_mean = Vars.down_path_2_block_2_running_mean;
state.down_path_2_block_2_running_var = Vars.down_path_2_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x247] = prepareConvArgs(Vars.down_path_2_block_3_weight, Vars.down_path_2_block_3_bias, Vars.ConvStride1024, Vars.ConvDilationFactor1025, Vars.ConvPadding1026, 1, NumDims.x246, NumDims.down_path_2_block_3_weight);
Vars.x247 = dlconv(Vars.x246, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x248 = relu(Vars.x247);
NumDims.x248 = NumDims.x247;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x249, NumDims.down_path_2_block_5_running_mean, NumDims.down_path_2_block_5_running_var] = prepareBatchNormalizationArgs(Vars.down_path_2_block_5_bias, Vars.down_path_2_block_5_weight, Vars.down_path_2_block_5_running_mean, Vars.down_path_2_block_5_running_var, NumDims.x248, NumDims.down_path_2_block_5_running_mean, NumDims.down_path_2_block_5_running_var);
if Training
    [Vars.x249, dsmean, dsvar] = batchnorm(Vars.x248, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_2_block_5_running_mean = dlarray(dsmean);
    Vars.down_path_2_block_5_running_var = dlarray(dsvar);
else
    Vars.x249 = batchnorm(Vars.x248, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_2_block_5_running_mean = Vars.down_path_2_block_5_running_mean;
state.down_path_2_block_5_running_var = Vars.down_path_2_block_5_running_var;

% Pad:
[Vars.x250, NumDims.x250] = onnxPad(Vars.x249, Vars.PadPadding1027, 0.000000, 'constant', NumDims.x249);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.x251] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1028, Vars.AveragePoolStride1029, Vars.AveragePoolPadding1030, 0, NumDims.x250);
Vars.x251 = avgpool(Vars.x250, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x252] = prepareConvArgs(Vars.down_path_3_block_0_weight, Vars.down_path_3_block_0_bias, Vars.ConvStride1031, Vars.ConvDilationFactor1032, Vars.ConvPadding1033, 1, NumDims.x251, NumDims.down_path_3_block_0_weight);
Vars.x252 = dlconv(Vars.x251, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x253 = relu(Vars.x252);
NumDims.x253 = NumDims.x252;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x254, NumDims.down_path_3_block_2_running_mean, NumDims.down_path_3_block_2_running_var] = prepareBatchNormalizationArgs(Vars.down_path_3_block_2_bias, Vars.down_path_3_block_2_weight, Vars.down_path_3_block_2_running_mean, Vars.down_path_3_block_2_running_var, NumDims.x253, NumDims.down_path_3_block_2_running_mean, NumDims.down_path_3_block_2_running_var);
if Training
    [Vars.x254, dsmean, dsvar] = batchnorm(Vars.x253, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_3_block_2_running_mean = dlarray(dsmean);
    Vars.down_path_3_block_2_running_var = dlarray(dsvar);
else
    Vars.x254 = batchnorm(Vars.x253, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_3_block_2_running_mean = Vars.down_path_3_block_2_running_mean;
state.down_path_3_block_2_running_var = Vars.down_path_3_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x255] = prepareConvArgs(Vars.down_path_3_block_3_weight, Vars.down_path_3_block_3_bias, Vars.ConvStride1034, Vars.ConvDilationFactor1035, Vars.ConvPadding1036, 1, NumDims.x254, NumDims.down_path_3_block_3_weight);
Vars.x255 = dlconv(Vars.x254, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x256 = relu(Vars.x255);
NumDims.x256 = NumDims.x255;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x257, NumDims.down_path_3_block_5_running_mean, NumDims.down_path_3_block_5_running_var] = prepareBatchNormalizationArgs(Vars.down_path_3_block_5_bias, Vars.down_path_3_block_5_weight, Vars.down_path_3_block_5_running_mean, Vars.down_path_3_block_5_running_var, NumDims.x256, NumDims.down_path_3_block_5_running_mean, NumDims.down_path_3_block_5_running_var);
if Training
    [Vars.x257, dsmean, dsvar] = batchnorm(Vars.x256, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_3_block_5_running_mean = dlarray(dsmean);
    Vars.down_path_3_block_5_running_var = dlarray(dsvar);
else
    Vars.x257 = batchnorm(Vars.x256, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_3_block_5_running_mean = Vars.down_path_3_block_5_running_mean;
state.down_path_3_block_5_running_var = Vars.down_path_3_block_5_running_var;

% Pad:
[Vars.x258, NumDims.x258] = onnxPad(Vars.x257, Vars.PadPadding1037, 0.000000, 'constant', NumDims.x257);

% AveragePool:
[poolSize, stride, padding, paddingValue, dataFormat, NumDims.x259] = prepareAveragePoolArgs(Vars.AveragePoolPoolSize1038, Vars.AveragePoolStride1039, Vars.AveragePoolPadding1040, 0, NumDims.x258);
Vars.x259 = avgpool(Vars.x258, poolSize, 'Stride', stride, 'Padding', padding, 'PaddingValue', paddingValue, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x260] = prepareConvArgs(Vars.down_path_4_block_0_weight, Vars.down_path_4_block_0_bias, Vars.ConvStride1041, Vars.ConvDilationFactor1042, Vars.ConvPadding1043, 1, NumDims.x259, NumDims.down_path_4_block_0_weight);
Vars.x260 = dlconv(Vars.x259, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x261 = relu(Vars.x260);
NumDims.x261 = NumDims.x260;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x262, NumDims.down_path_4_block_2_running_mean, NumDims.down_path_4_block_2_running_var] = prepareBatchNormalizationArgs(Vars.down_path_4_block_2_bias, Vars.down_path_4_block_2_weight, Vars.down_path_4_block_2_running_mean, Vars.down_path_4_block_2_running_var, NumDims.x261, NumDims.down_path_4_block_2_running_mean, NumDims.down_path_4_block_2_running_var);
if Training
    [Vars.x262, dsmean, dsvar] = batchnorm(Vars.x261, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_4_block_2_running_mean = dlarray(dsmean);
    Vars.down_path_4_block_2_running_var = dlarray(dsvar);
else
    Vars.x262 = batchnorm(Vars.x261, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_4_block_2_running_mean = Vars.down_path_4_block_2_running_mean;
state.down_path_4_block_2_running_var = Vars.down_path_4_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x263] = prepareConvArgs(Vars.down_path_4_block_3_weight, Vars.down_path_4_block_3_bias, Vars.ConvStride1044, Vars.ConvDilationFactor1045, Vars.ConvPadding1046, 1, NumDims.x262, NumDims.down_path_4_block_3_weight);
Vars.x263 = dlconv(Vars.x262, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x264 = relu(Vars.x263);
NumDims.x264 = NumDims.x263;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x265, NumDims.down_path_4_block_5_running_mean, NumDims.down_path_4_block_5_running_var] = prepareBatchNormalizationArgs(Vars.down_path_4_block_5_bias, Vars.down_path_4_block_5_weight, Vars.down_path_4_block_5_running_mean, Vars.down_path_4_block_5_running_var, NumDims.x264, NumDims.down_path_4_block_5_running_mean, NumDims.down_path_4_block_5_running_var);
if Training
    [Vars.x265, dsmean, dsvar] = batchnorm(Vars.x264, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.down_path_4_block_5_running_mean = dlarray(dsmean);
    Vars.down_path_4_block_5_running_var = dlarray(dsvar);
else
    Vars.x265 = batchnorm(Vars.x264, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.down_path_4_block_5_running_mean = Vars.down_path_4_block_5_running_mean;
state.down_path_4_block_5_running_var = Vars.down_path_4_block_5_running_var;

% Resize:
[DLTScales, dataFormat, Method, NumDims.x269] = prepareResize10Args(Vars.x462, "linear", NumDims.x265);
Vars.x269 = dlresize(Vars.x265, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', 'asymmetric', 'NearestRoundingMode', 'onnx-10');

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x270] = prepareConvArgs(Vars.up_path_0_up_1_weight, Vars.up_path_0_up_1_bias, Vars.ConvStride1047, Vars.ConvDilationFactor1048, Vars.ConvPadding1049, 1, NumDims.x269, NumDims.up_path_0_up_1_weight);
Vars.x270 = dlconv(Vars.x269, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Shape:
[Vars.x271, NumDims.x271] = onnxShape(Vars.x270, NumDims.x270);

% Gather:
[Vars.x273, NumDims.x273] = onnxGather(Vars.x271, Vars.x272, 0, NumDims.x271, NumDims.x272);

% Shape:
[Vars.x274, NumDims.x274] = onnxShape(Vars.x270, NumDims.x270);

% Gather:
[Vars.x276, NumDims.x276] = onnxGather(Vars.x274, Vars.x275, 0, NumDims.x274, NumDims.x275);

% Shape:
[Vars.x277, NumDims.x277] = onnxShape(Vars.x257, NumDims.x257);

% Gather:
[Vars.x279, NumDims.x279] = onnxGather(Vars.x277, Vars.x278, 0, NumDims.x277, NumDims.x278);

% Shape:
[Vars.x280, NumDims.x280] = onnxShape(Vars.x257, NumDims.x257);

% Gather:
[Vars.x282, NumDims.x282] = onnxGather(Vars.x280, Vars.x281, 0, NumDims.x280, NumDims.x281);

% Sub:
Vars.x283 = Vars.x279 - Vars.x273;
NumDims.x283 = max(NumDims.x279, NumDims.x273);

% Div:
Vars.x285 = fix(Vars.x283 ./ Vars.x284);
NumDims.x285 = max(NumDims.x283, NumDims.x284);

% Cast:
Vars.x286 = cast(int64(extractdata(Vars.x285)), 'like', Vars.x285);
NumDims.x286 = NumDims.x285;

% Cast:
Vars.x287 = cast(int64(extractdata(Vars.x286)), 'like', Vars.x286);
NumDims.x287 = NumDims.x286;

% Sub:
Vars.x288 = Vars.x282 - Vars.x276;
NumDims.x288 = max(NumDims.x282, NumDims.x276);

% Div:
Vars.x290 = fix(Vars.x288 ./ Vars.x289);
NumDims.x290 = max(NumDims.x288, NumDims.x289);

% Cast:
Vars.x291 = cast(int64(extractdata(Vars.x290)), 'like', Vars.x290);
NumDims.x291 = NumDims.x290;

% Cast:
Vars.x292 = cast(int64(extractdata(Vars.x291)), 'like', Vars.x291);
NumDims.x292 = NumDims.x291;

% Add:
Vars.x293 = Vars.x287 + Vars.x273;
NumDims.x293 = max(NumDims.x287, NumDims.x273);

% Add:
Vars.x294 = Vars.x292 + Vars.x276;
NumDims.x294 = max(NumDims.x292, NumDims.x276);

% Unsqueeze:
[shape, NumDims.x296] = prepareUnsqueezeArgs(Vars.x287, Vars.UnsqueezeAxes1050, NumDims.x287);
Vars.x296 = reshape(Vars.x287, shape);

% Unsqueeze:
[shape, NumDims.x297] = prepareUnsqueezeArgs(Vars.x293, Vars.UnsqueezeAxes1051, NumDims.x293);
Vars.x297 = reshape(Vars.x293, shape);

% Slice:
[Indices, NumDims.x300] = prepareSliceArgs(Vars.x257, Vars.x296, Vars.x297, Vars.x463, Vars.x299, NumDims.x257);
Vars.x300 = subsref(Vars.x257, Indices);

% Unsqueeze:
[shape, NumDims.x302] = prepareUnsqueezeArgs(Vars.x292, Vars.UnsqueezeAxes1052, NumDims.x292);
Vars.x302 = reshape(Vars.x292, shape);

% Unsqueeze:
[shape, NumDims.x303] = prepareUnsqueezeArgs(Vars.x294, Vars.UnsqueezeAxes1053, NumDims.x294);
Vars.x303 = reshape(Vars.x294, shape);

% Slice:
[Indices, NumDims.x306] = prepareSliceArgs(Vars.x300, Vars.x302, Vars.x303, Vars.x464, Vars.x305, NumDims.x300);
Vars.x306 = subsref(Vars.x300, Indices);

% Concat:
[Vars.x307, NumDims.x307] = onnxConcat(1, {Vars.x270, Vars.x306}, [NumDims.x270, NumDims.x306]);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x308] = prepareConvArgs(Vars.up_path_0_conv_block_block_0_weight, Vars.up_path_0_conv_block_block_0_bias, Vars.ConvStride1054, Vars.ConvDilationFactor1055, Vars.ConvPadding1056, 1, NumDims.x307, NumDims.up_path_0_conv_block_block_0_weight);
Vars.x308 = dlconv(Vars.x307, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x309 = relu(Vars.x308);
NumDims.x309 = NumDims.x308;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x310, NumDims.up_path_0_conv_block_block_2_running_mea, NumDims.up_path_0_conv_block_block_2_running_var] = prepareBatchNormalizationArgs(Vars.up_path_0_conv_block_block_2_bias, Vars.up_path_0_conv_block_block_2_weight, Vars.up_path_0_conv_block_block_2_running_mea, Vars.up_path_0_conv_block_block_2_running_var, NumDims.x309, NumDims.up_path_0_conv_block_block_2_running_mea, NumDims.up_path_0_conv_block_block_2_running_var);
if Training
    [Vars.x310, dsmean, dsvar] = batchnorm(Vars.x309, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_0_conv_block_block_2_running_mea = dlarray(dsmean);
    Vars.up_path_0_conv_block_block_2_running_var = dlarray(dsvar);
else
    Vars.x310 = batchnorm(Vars.x309, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_0_conv_block_block_2_running_mea = Vars.up_path_0_conv_block_block_2_running_mea;
state.up_path_0_conv_block_block_2_running_var = Vars.up_path_0_conv_block_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x311] = prepareConvArgs(Vars.up_path_0_conv_block_block_3_weight, Vars.up_path_0_conv_block_block_3_bias, Vars.ConvStride1057, Vars.ConvDilationFactor1058, Vars.ConvPadding1059, 1, NumDims.x310, NumDims.up_path_0_conv_block_block_3_weight);
Vars.x311 = dlconv(Vars.x310, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x312 = relu(Vars.x311);
NumDims.x312 = NumDims.x311;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x313, NumDims.up_path_0_conv_block_block_5_running_mea, NumDims.up_path_0_conv_block_block_5_running_var] = prepareBatchNormalizationArgs(Vars.up_path_0_conv_block_block_5_bias, Vars.up_path_0_conv_block_block_5_weight, Vars.up_path_0_conv_block_block_5_running_mea, Vars.up_path_0_conv_block_block_5_running_var, NumDims.x312, NumDims.up_path_0_conv_block_block_5_running_mea, NumDims.up_path_0_conv_block_block_5_running_var);
if Training
    [Vars.x313, dsmean, dsvar] = batchnorm(Vars.x312, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_0_conv_block_block_5_running_mea = dlarray(dsmean);
    Vars.up_path_0_conv_block_block_5_running_var = dlarray(dsvar);
else
    Vars.x313 = batchnorm(Vars.x312, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_0_conv_block_block_5_running_mea = Vars.up_path_0_conv_block_block_5_running_mea;
state.up_path_0_conv_block_block_5_running_var = Vars.up_path_0_conv_block_block_5_running_var;

% Resize:
[DLTScales, dataFormat, Method, NumDims.x317] = prepareResize10Args(Vars.x465, "linear", NumDims.x313);
Vars.x317 = dlresize(Vars.x313, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', 'asymmetric', 'NearestRoundingMode', 'onnx-10');

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x318] = prepareConvArgs(Vars.up_path_1_up_1_weight, Vars.up_path_1_up_1_bias, Vars.ConvStride1060, Vars.ConvDilationFactor1061, Vars.ConvPadding1062, 1, NumDims.x317, NumDims.up_path_1_up_1_weight);
Vars.x318 = dlconv(Vars.x317, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Shape:
[Vars.x319, NumDims.x319] = onnxShape(Vars.x318, NumDims.x318);

% Gather:
[Vars.x321, NumDims.x321] = onnxGather(Vars.x319, Vars.x320, 0, NumDims.x319, NumDims.x320);

% Shape:
[Vars.x322, NumDims.x322] = onnxShape(Vars.x318, NumDims.x318);

% Gather:
[Vars.x324, NumDims.x324] = onnxGather(Vars.x322, Vars.x323, 0, NumDims.x322, NumDims.x323);

% Shape:
[Vars.x325, NumDims.x325] = onnxShape(Vars.x249, NumDims.x249);

% Gather:
[Vars.x327, NumDims.x327] = onnxGather(Vars.x325, Vars.x326, 0, NumDims.x325, NumDims.x326);

% Shape:
[Vars.x328, NumDims.x328] = onnxShape(Vars.x249, NumDims.x249);

% Gather:
[Vars.x330, NumDims.x330] = onnxGather(Vars.x328, Vars.x329, 0, NumDims.x328, NumDims.x329);

% Sub:
Vars.x331 = Vars.x327 - Vars.x321;
NumDims.x331 = max(NumDims.x327, NumDims.x321);

% Div:
Vars.x333 = fix(Vars.x331 ./ Vars.x332);
NumDims.x333 = max(NumDims.x331, NumDims.x332);

% Cast:
Vars.x334 = cast(int64(extractdata(Vars.x333)), 'like', Vars.x333);
NumDims.x334 = NumDims.x333;

% Cast:
Vars.x335 = cast(int64(extractdata(Vars.x334)), 'like', Vars.x334);
NumDims.x335 = NumDims.x334;

% Sub:
Vars.x336 = Vars.x330 - Vars.x324;
NumDims.x336 = max(NumDims.x330, NumDims.x324);

% Div:
Vars.x338 = fix(Vars.x336 ./ Vars.x337);
NumDims.x338 = max(NumDims.x336, NumDims.x337);

% Cast:
Vars.x339 = cast(int64(extractdata(Vars.x338)), 'like', Vars.x338);
NumDims.x339 = NumDims.x338;

% Cast:
Vars.x340 = cast(int64(extractdata(Vars.x339)), 'like', Vars.x339);
NumDims.x340 = NumDims.x339;

% Add:
Vars.x341 = Vars.x335 + Vars.x321;
NumDims.x341 = max(NumDims.x335, NumDims.x321);

% Add:
Vars.x342 = Vars.x340 + Vars.x324;
NumDims.x342 = max(NumDims.x340, NumDims.x324);

% Unsqueeze:
[shape, NumDims.x344] = prepareUnsqueezeArgs(Vars.x335, Vars.UnsqueezeAxes1063, NumDims.x335);
Vars.x344 = reshape(Vars.x335, shape);

% Unsqueeze:
[shape, NumDims.x345] = prepareUnsqueezeArgs(Vars.x341, Vars.UnsqueezeAxes1064, NumDims.x341);
Vars.x345 = reshape(Vars.x341, shape);

% Slice:
[Indices, NumDims.x348] = prepareSliceArgs(Vars.x249, Vars.x344, Vars.x345, Vars.x466, Vars.x347, NumDims.x249);
Vars.x348 = subsref(Vars.x249, Indices);

% Unsqueeze:
[shape, NumDims.x350] = prepareUnsqueezeArgs(Vars.x340, Vars.UnsqueezeAxes1065, NumDims.x340);
Vars.x350 = reshape(Vars.x340, shape);

% Unsqueeze:
[shape, NumDims.x351] = prepareUnsqueezeArgs(Vars.x342, Vars.UnsqueezeAxes1066, NumDims.x342);
Vars.x351 = reshape(Vars.x342, shape);

% Slice:
[Indices, NumDims.x354] = prepareSliceArgs(Vars.x348, Vars.x350, Vars.x351, Vars.x467, Vars.x353, NumDims.x348);
Vars.x354 = subsref(Vars.x348, Indices);

% Concat:
[Vars.x355, NumDims.x355] = onnxConcat(1, {Vars.x318, Vars.x354}, [NumDims.x318, NumDims.x354]);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x356] = prepareConvArgs(Vars.up_path_1_conv_block_block_0_weight, Vars.up_path_1_conv_block_block_0_bias, Vars.ConvStride1067, Vars.ConvDilationFactor1068, Vars.ConvPadding1069, 1, NumDims.x355, NumDims.up_path_1_conv_block_block_0_weight);
Vars.x356 = dlconv(Vars.x355, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x357 = relu(Vars.x356);
NumDims.x357 = NumDims.x356;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x358, NumDims.up_path_1_conv_block_block_2_running_mea, NumDims.up_path_1_conv_block_block_2_running_var] = prepareBatchNormalizationArgs(Vars.up_path_1_conv_block_block_2_bias, Vars.up_path_1_conv_block_block_2_weight, Vars.up_path_1_conv_block_block_2_running_mea, Vars.up_path_1_conv_block_block_2_running_var, NumDims.x357, NumDims.up_path_1_conv_block_block_2_running_mea, NumDims.up_path_1_conv_block_block_2_running_var);
if Training
    [Vars.x358, dsmean, dsvar] = batchnorm(Vars.x357, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_1_conv_block_block_2_running_mea = dlarray(dsmean);
    Vars.up_path_1_conv_block_block_2_running_var = dlarray(dsvar);
else
    Vars.x358 = batchnorm(Vars.x357, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_1_conv_block_block_2_running_mea = Vars.up_path_1_conv_block_block_2_running_mea;
state.up_path_1_conv_block_block_2_running_var = Vars.up_path_1_conv_block_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x359] = prepareConvArgs(Vars.up_path_1_conv_block_block_3_weight, Vars.up_path_1_conv_block_block_3_bias, Vars.ConvStride1070, Vars.ConvDilationFactor1071, Vars.ConvPadding1072, 1, NumDims.x358, NumDims.up_path_1_conv_block_block_3_weight);
Vars.x359 = dlconv(Vars.x358, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x360 = relu(Vars.x359);
NumDims.x360 = NumDims.x359;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x361, NumDims.up_path_1_conv_block_block_5_running_mea, NumDims.up_path_1_conv_block_block_5_running_var] = prepareBatchNormalizationArgs(Vars.up_path_1_conv_block_block_5_bias, Vars.up_path_1_conv_block_block_5_weight, Vars.up_path_1_conv_block_block_5_running_mea, Vars.up_path_1_conv_block_block_5_running_var, NumDims.x360, NumDims.up_path_1_conv_block_block_5_running_mea, NumDims.up_path_1_conv_block_block_5_running_var);
if Training
    [Vars.x361, dsmean, dsvar] = batchnorm(Vars.x360, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_1_conv_block_block_5_running_mea = dlarray(dsmean);
    Vars.up_path_1_conv_block_block_5_running_var = dlarray(dsvar);
else
    Vars.x361 = batchnorm(Vars.x360, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_1_conv_block_block_5_running_mea = Vars.up_path_1_conv_block_block_5_running_mea;
state.up_path_1_conv_block_block_5_running_var = Vars.up_path_1_conv_block_block_5_running_var;

% Resize:
[DLTScales, dataFormat, Method, NumDims.x365] = prepareResize10Args(Vars.x468, "linear", NumDims.x361);
Vars.x365 = dlresize(Vars.x361, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', 'asymmetric', 'NearestRoundingMode', 'onnx-10');

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x366] = prepareConvArgs(Vars.up_path_2_up_1_weight, Vars.up_path_2_up_1_bias, Vars.ConvStride1073, Vars.ConvDilationFactor1074, Vars.ConvPadding1075, 1, NumDims.x365, NumDims.up_path_2_up_1_weight);
Vars.x366 = dlconv(Vars.x365, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Shape:
[Vars.x367, NumDims.x367] = onnxShape(Vars.x366, NumDims.x366);

% Gather:
[Vars.x369, NumDims.x369] = onnxGather(Vars.x367, Vars.x368, 0, NumDims.x367, NumDims.x368);

% Shape:
[Vars.x370, NumDims.x370] = onnxShape(Vars.x366, NumDims.x366);

% Gather:
[Vars.x372, NumDims.x372] = onnxGather(Vars.x370, Vars.x371, 0, NumDims.x370, NumDims.x371);

% Shape:
[Vars.x373, NumDims.x373] = onnxShape(Vars.x241, NumDims.x241);

% Gather:
[Vars.x375, NumDims.x375] = onnxGather(Vars.x373, Vars.x374, 0, NumDims.x373, NumDims.x374);

% Shape:
[Vars.x376, NumDims.x376] = onnxShape(Vars.x241, NumDims.x241);

% Gather:
[Vars.x378, NumDims.x378] = onnxGather(Vars.x376, Vars.x377, 0, NumDims.x376, NumDims.x377);

% Sub:
Vars.x379 = Vars.x375 - Vars.x369;
NumDims.x379 = max(NumDims.x375, NumDims.x369);

% Div:
Vars.x381 = fix(Vars.x379 ./ Vars.x380);
NumDims.x381 = max(NumDims.x379, NumDims.x380);

% Cast:
Vars.x382 = cast(int64(extractdata(Vars.x381)), 'like', Vars.x381);
NumDims.x382 = NumDims.x381;

% Cast:
Vars.x383 = cast(int64(extractdata(Vars.x382)), 'like', Vars.x382);
NumDims.x383 = NumDims.x382;

% Sub:
Vars.x384 = Vars.x378 - Vars.x372;
NumDims.x384 = max(NumDims.x378, NumDims.x372);

% Div:
Vars.x386 = fix(Vars.x384 ./ Vars.x385);
NumDims.x386 = max(NumDims.x384, NumDims.x385);

% Cast:
Vars.x387 = cast(int64(extractdata(Vars.x386)), 'like', Vars.x386);
NumDims.x387 = NumDims.x386;

% Cast:
Vars.x388 = cast(int64(extractdata(Vars.x387)), 'like', Vars.x387);
NumDims.x388 = NumDims.x387;

% Add:
Vars.x389 = Vars.x383 + Vars.x369;
NumDims.x389 = max(NumDims.x383, NumDims.x369);

% Add:
Vars.x390 = Vars.x388 + Vars.x372;
NumDims.x390 = max(NumDims.x388, NumDims.x372);

% Unsqueeze:
[shape, NumDims.x392] = prepareUnsqueezeArgs(Vars.x383, Vars.UnsqueezeAxes1076, NumDims.x383);
Vars.x392 = reshape(Vars.x383, shape);

% Unsqueeze:
[shape, NumDims.x393] = prepareUnsqueezeArgs(Vars.x389, Vars.UnsqueezeAxes1077, NumDims.x389);
Vars.x393 = reshape(Vars.x389, shape);

% Slice:
[Indices, NumDims.x396] = prepareSliceArgs(Vars.x241, Vars.x392, Vars.x393, Vars.x469, Vars.x395, NumDims.x241);
Vars.x396 = subsref(Vars.x241, Indices);

% Unsqueeze:
[shape, NumDims.x398] = prepareUnsqueezeArgs(Vars.x388, Vars.UnsqueezeAxes1078, NumDims.x388);
Vars.x398 = reshape(Vars.x388, shape);

% Unsqueeze:
[shape, NumDims.x399] = prepareUnsqueezeArgs(Vars.x390, Vars.UnsqueezeAxes1079, NumDims.x390);
Vars.x399 = reshape(Vars.x390, shape);

% Slice:
[Indices, NumDims.x402] = prepareSliceArgs(Vars.x396, Vars.x398, Vars.x399, Vars.x470, Vars.x401, NumDims.x396);
Vars.x402 = subsref(Vars.x396, Indices);

% Concat:
[Vars.x403, NumDims.x403] = onnxConcat(1, {Vars.x366, Vars.x402}, [NumDims.x366, NumDims.x402]);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x404] = prepareConvArgs(Vars.up_path_2_conv_block_block_0_weight, Vars.up_path_2_conv_block_block_0_bias, Vars.ConvStride1080, Vars.ConvDilationFactor1081, Vars.ConvPadding1082, 1, NumDims.x403, NumDims.up_path_2_conv_block_block_0_weight);
Vars.x404 = dlconv(Vars.x403, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x405 = relu(Vars.x404);
NumDims.x405 = NumDims.x404;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x406, NumDims.up_path_2_conv_block_block_2_running_mea, NumDims.up_path_2_conv_block_block_2_running_var] = prepareBatchNormalizationArgs(Vars.up_path_2_conv_block_block_2_bias, Vars.up_path_2_conv_block_block_2_weight, Vars.up_path_2_conv_block_block_2_running_mea, Vars.up_path_2_conv_block_block_2_running_var, NumDims.x405, NumDims.up_path_2_conv_block_block_2_running_mea, NumDims.up_path_2_conv_block_block_2_running_var);
if Training
    [Vars.x406, dsmean, dsvar] = batchnorm(Vars.x405, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_2_conv_block_block_2_running_mea = dlarray(dsmean);
    Vars.up_path_2_conv_block_block_2_running_var = dlarray(dsvar);
else
    Vars.x406 = batchnorm(Vars.x405, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_2_conv_block_block_2_running_mea = Vars.up_path_2_conv_block_block_2_running_mea;
state.up_path_2_conv_block_block_2_running_var = Vars.up_path_2_conv_block_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x407] = prepareConvArgs(Vars.up_path_2_conv_block_block_3_weight, Vars.up_path_2_conv_block_block_3_bias, Vars.ConvStride1083, Vars.ConvDilationFactor1084, Vars.ConvPadding1085, 1, NumDims.x406, NumDims.up_path_2_conv_block_block_3_weight);
Vars.x407 = dlconv(Vars.x406, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x408 = relu(Vars.x407);
NumDims.x408 = NumDims.x407;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x409, NumDims.up_path_2_conv_block_block_5_running_mea, NumDims.up_path_2_conv_block_block_5_running_var] = prepareBatchNormalizationArgs(Vars.up_path_2_conv_block_block_5_bias, Vars.up_path_2_conv_block_block_5_weight, Vars.up_path_2_conv_block_block_5_running_mea, Vars.up_path_2_conv_block_block_5_running_var, NumDims.x408, NumDims.up_path_2_conv_block_block_5_running_mea, NumDims.up_path_2_conv_block_block_5_running_var);
if Training
    [Vars.x409, dsmean, dsvar] = batchnorm(Vars.x408, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_2_conv_block_block_5_running_mea = dlarray(dsmean);
    Vars.up_path_2_conv_block_block_5_running_var = dlarray(dsvar);
else
    Vars.x409 = batchnorm(Vars.x408, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_2_conv_block_block_5_running_mea = Vars.up_path_2_conv_block_block_5_running_mea;
state.up_path_2_conv_block_block_5_running_var = Vars.up_path_2_conv_block_block_5_running_var;

% Resize:
[DLTScales, dataFormat, Method, NumDims.x413] = prepareResize10Args(Vars.x471, "linear", NumDims.x409);
Vars.x413 = dlresize(Vars.x409, 'Scale', DLTScales, 'DataFormat', dataFormat, 'Method', Method, 'GeometricTransformMode', 'asymmetric', 'NearestRoundingMode', 'onnx-10');

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x414] = prepareConvArgs(Vars.up_path_3_up_1_weight, Vars.up_path_3_up_1_bias, Vars.ConvStride1086, Vars.ConvDilationFactor1087, Vars.ConvPadding1088, 1, NumDims.x413, NumDims.up_path_3_up_1_weight);
Vars.x414 = dlconv(Vars.x413, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Shape:
[Vars.x415, NumDims.x415] = onnxShape(Vars.x414, NumDims.x414);

% Gather:
[Vars.x417, NumDims.x417] = onnxGather(Vars.x415, Vars.x416, 0, NumDims.x415, NumDims.x416);

% Shape:
[Vars.x418, NumDims.x418] = onnxShape(Vars.x414, NumDims.x414);

% Gather:
[Vars.x420, NumDims.x420] = onnxGather(Vars.x418, Vars.x419, 0, NumDims.x418, NumDims.x419);

% Shape:
[Vars.x421, NumDims.x421] = onnxShape(Vars.x233, NumDims.x233);

% Gather:
[Vars.x423, NumDims.x423] = onnxGather(Vars.x421, Vars.x422, 0, NumDims.x421, NumDims.x422);

% Shape:
[Vars.x424, NumDims.x424] = onnxShape(Vars.x233, NumDims.x233);

% Gather:
[Vars.x426, NumDims.x426] = onnxGather(Vars.x424, Vars.x425, 0, NumDims.x424, NumDims.x425);

% Sub:
Vars.x427 = Vars.x423 - Vars.x417;
NumDims.x427 = max(NumDims.x423, NumDims.x417);

% Div:
Vars.x429 = fix(Vars.x427 ./ Vars.x428);
NumDims.x429 = max(NumDims.x427, NumDims.x428);

% Cast:
Vars.x430 = cast(int64(extractdata(Vars.x429)), 'like', Vars.x429);
NumDims.x430 = NumDims.x429;

% Cast:
Vars.x431 = cast(int64(extractdata(Vars.x430)), 'like', Vars.x430);
NumDims.x431 = NumDims.x430;

% Sub:
Vars.x432 = Vars.x426 - Vars.x420;
NumDims.x432 = max(NumDims.x426, NumDims.x420);

% Div:
Vars.x434 = fix(Vars.x432 ./ Vars.x433);
NumDims.x434 = max(NumDims.x432, NumDims.x433);

% Cast:
Vars.x435 = cast(int64(extractdata(Vars.x434)), 'like', Vars.x434);
NumDims.x435 = NumDims.x434;

% Cast:
Vars.x436 = cast(int64(extractdata(Vars.x435)), 'like', Vars.x435);
NumDims.x436 = NumDims.x435;

% Add:
Vars.x437 = Vars.x431 + Vars.x417;
NumDims.x437 = max(NumDims.x431, NumDims.x417);

% Add:
Vars.x438 = Vars.x436 + Vars.x420;
NumDims.x438 = max(NumDims.x436, NumDims.x420);

% Unsqueeze:
[shape, NumDims.x440] = prepareUnsqueezeArgs(Vars.x431, Vars.UnsqueezeAxes1089, NumDims.x431);
Vars.x440 = reshape(Vars.x431, shape);

% Unsqueeze:
[shape, NumDims.x441] = prepareUnsqueezeArgs(Vars.x437, Vars.UnsqueezeAxes1090, NumDims.x437);
Vars.x441 = reshape(Vars.x437, shape);

% Slice:
[Indices, NumDims.x444] = prepareSliceArgs(Vars.x233, Vars.x440, Vars.x441, Vars.x472, Vars.x443, NumDims.x233);
Vars.x444 = subsref(Vars.x233, Indices);

% Unsqueeze:
[shape, NumDims.x446] = prepareUnsqueezeArgs(Vars.x436, Vars.UnsqueezeAxes1091, NumDims.x436);
Vars.x446 = reshape(Vars.x436, shape);

% Unsqueeze:
[shape, NumDims.x447] = prepareUnsqueezeArgs(Vars.x438, Vars.UnsqueezeAxes1092, NumDims.x438);
Vars.x447 = reshape(Vars.x438, shape);

% Slice:
[Indices, NumDims.x450] = prepareSliceArgs(Vars.x444, Vars.x446, Vars.x447, Vars.x473, Vars.x449, NumDims.x444);
Vars.x450 = subsref(Vars.x444, Indices);

% Concat:
[Vars.x451, NumDims.x451] = onnxConcat(1, {Vars.x414, Vars.x450}, [NumDims.x414, NumDims.x450]);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x452] = prepareConvArgs(Vars.up_path_3_conv_block_block_0_weight, Vars.up_path_3_conv_block_block_0_bias, Vars.ConvStride1093, Vars.ConvDilationFactor1094, Vars.ConvPadding1095, 1, NumDims.x451, NumDims.up_path_3_conv_block_block_0_weight);
Vars.x452 = dlconv(Vars.x451, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x453 = relu(Vars.x452);
NumDims.x453 = NumDims.x452;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x454, NumDims.up_path_3_conv_block_block_2_running_mea, NumDims.up_path_3_conv_block_block_2_running_var] = prepareBatchNormalizationArgs(Vars.up_path_3_conv_block_block_2_bias, Vars.up_path_3_conv_block_block_2_weight, Vars.up_path_3_conv_block_block_2_running_mea, Vars.up_path_3_conv_block_block_2_running_var, NumDims.x453, NumDims.up_path_3_conv_block_block_2_running_mea, NumDims.up_path_3_conv_block_block_2_running_var);
if Training
    [Vars.x454, dsmean, dsvar] = batchnorm(Vars.x453, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_3_conv_block_block_2_running_mea = dlarray(dsmean);
    Vars.up_path_3_conv_block_block_2_running_var = dlarray(dsvar);
else
    Vars.x454 = batchnorm(Vars.x453, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_3_conv_block_block_2_running_mea = Vars.up_path_3_conv_block_block_2_running_mea;
state.up_path_3_conv_block_block_2_running_var = Vars.up_path_3_conv_block_block_2_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x455] = prepareConvArgs(Vars.up_path_3_conv_block_block_3_weight, Vars.up_path_3_conv_block_block_3_bias, Vars.ConvStride1096, Vars.ConvDilationFactor1097, Vars.ConvPadding1098, 1, NumDims.x454, NumDims.up_path_3_conv_block_block_3_weight);
Vars.x455 = dlconv(Vars.x454, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Relu:
Vars.x456 = relu(Vars.x455);
NumDims.x456 = NumDims.x455;

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x457, NumDims.up_path_3_conv_block_block_5_running_mea, NumDims.up_path_3_conv_block_block_5_running_var] = prepareBatchNormalizationArgs(Vars.up_path_3_conv_block_block_5_bias, Vars.up_path_3_conv_block_block_5_weight, Vars.up_path_3_conv_block_block_5_running_mea, Vars.up_path_3_conv_block_block_5_running_var, NumDims.x456, NumDims.up_path_3_conv_block_block_5_running_mea, NumDims.up_path_3_conv_block_block_5_running_var);
if Training
    [Vars.x457, dsmean, dsvar] = batchnorm(Vars.x456, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.up_path_3_conv_block_block_5_running_mea = dlarray(dsmean);
    Vars.up_path_3_conv_block_block_5_running_var = dlarray(dsvar);
else
    Vars.x457 = batchnorm(Vars.x456, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.up_path_3_conv_block_block_5_running_mea = Vars.up_path_3_conv_block_block_5_running_mea;
state.up_path_3_conv_block_block_5_running_var = Vars.up_path_3_conv_block_block_5_running_var;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x458] = prepareConvArgs(Vars.last_weight, Vars.last_bias, Vars.ConvStride1099, Vars.ConvDilationFactor1100, Vars.ConvPadding1101, 1, NumDims.x457, NumDims.last_weight);
Vars.x458 = dlconv(Vars.x457, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% Transpose:
[perm, NumDims.x459] = prepareTransposeArgs(Vars.TransposePerm1102, NumDims.x458);
if ~isempty(perm)
    Vars.x459 = permute(Vars.x458, perm);
end

% Replacement for PLACEHOLDER FUNCTION FOR UNSUPPORTED OPERATOR (LogSoftmax):
Vars.x460 = log(softmax(Vars.x459,'DataFormat','CSSB'));
NumDims.x460 = NumDims.x459;

% Transpose:
[perm, NumDims.output] = prepareTransposeArgs(Vars.TransposePerm1103, NumDims.x460);
if ~isempty(perm)
    Vars.output = permute(Vars.x460, perm);
end

% Set graph output arguments from Vars and NumDims:
output = Vars.output;
outputNumDims1105 = NumDims.output;
% Set output state from Vars:
state = updateStruct(state, Vars);
end

function [inputDataPerms, outputDataPerms, Training] = parseInputs(input, numDataOutputs, params, varargin)
% Function to validate inputs to importedLungmaskFcn_R231:
p = inputParser;
isValidArrayInput = @(x)isnumeric(x) || isstring(x);
isValidONNXParameters = @(x)isa(x, 'ONNXParameters');
addRequired(p, 'input', isValidArrayInput);
addRequired(p, 'params', isValidONNXParameters);
addParameter(p, 'InputDataPermutation', 'auto');
addParameter(p, 'OutputDataPermutation', 'auto');
addParameter(p, 'Training', false);
parse(p, input, params, varargin{:});
inputDataPerms = p.Results.InputDataPermutation;
outputDataPerms = p.Results.OutputDataPermutation;
Training = p.Results.Training;
if isnumeric(inputDataPerms)
    inputDataPerms = {inputDataPerms};
end
if isstring(inputDataPerms) && isscalar(inputDataPerms) || ischar(inputDataPerms)
    inputDataPerms = repmat({inputDataPerms},1,1);
end
if isnumeric(outputDataPerms)
    outputDataPerms = {outputDataPerms};
end
if isstring(outputDataPerms) && isscalar(outputDataPerms) || ischar(outputDataPerms)
    outputDataPerms = repmat({outputDataPerms},1,numDataOutputs);
end
end

function [input, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(input, params, varargin)
% Parse input arguments
[inputDataPerms, outputDataPerms, Training] = parseInputs(input, 1, params, varargin{:});
anyDlarrayInputs = any(cellfun(@(x)isa(x, 'dlarray'), {input}));
% Make the input variables into unlabelled dlarrays:
input = makeUnlabeledDlarray(input);
% Permute inputs if requested:
input = permuteInputVar(input, inputDataPerms{1}, 4);
% Check input size(s):
checkInputSize(size(input), {'batch_size' 1 256 256}, "input");
end

function [output] = postprocessOutput(output, outputDataPerms, anyDlarrayInputs, Training, varargin)
% Set output type:
if ~anyDlarrayInputs && ~Training
    if isdlarray(output)
        output = extractdata(output);
    end
end
% Permute outputs if requested:
output = permuteOutputVar(output, outputDataPerms{1}, 4);
end


%% dlarray functions implementing ONNX operators:

function [Y, numDimsY] = onnxConcat(ONNXAxis, XCell, numDimsXArray)
% Concatentation that treats all empties the same. Necessary because
% dlarray.cat does not allow, for example, cat(1, 1x1, 1x0) because the
% second dimension sizes do not match.
numDimsY = numDimsXArray(1);
XCell(cellfun(@isempty, XCell)) = [];
if isempty(XCell)
    Y = dlarray([]);
else
    if ONNXAxis<0
        ONNXAxis = ONNXAxis + numDimsY;
    end
    DLTAxis = numDimsY - ONNXAxis;
    Y = cat(DLTAxis, XCell{:});
end
end

function [Y, numDimsY] = onnxGather(X, ONNXIdx, ONNXAxis, numDimsX, numDimsIdx)
% Function implementing the ONNX Gather operator

% In ONNX, 'Gather' first indexes into dimension ONNXAxis of data, using
% the contents of ONNXIdx as the indices. Then, it reshapes the ONNXAxis
% into the shape of ONNXIdx.
%   Example 1:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6 7], and axis=1.
% The result has shape [2 6 7 4 5].
%   Example 2:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6], and axis=1.
% The result has shape [2 6 4 5].
%   Example 3:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [] (a scalar), and axis=1.
% The result has shape [2 4 5].
%
% Since we're using reverse indexing relative to ONNX, in this function
% data and ONNXIdx both have reversed dimension ordering.
numDimsY = numDimsIdx + (numDimsX - 1);
if isempty(X)
    Y = X;
    return;
end
% (1) First, do the subsref part of Gather
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsX;                                 % Axis can be negative. Convert it to its positive equivalent.
end
dltAxis = numDimsX - ONNXAxis;                                      % Convert axis to DLT. ONNXAxis is origin 0 and we index from the end
ONNXIdx(ONNXIdx<0) = ONNXIdx(ONNXIdx<0) + size(X, dltAxis);         % ONNXIdx can have negative components. Make them positive.
dltIdx  = extractdata(ONNXIdx) + 1;                                 % ONNXIdx is origin-0 in ONNX, so add 1 to get dltIdx
% Use subsref to index into data
Indices.subs = repmat({':'}, 1, numDimsX);
Indices.subs{dltAxis} = dltIdx(:);                                  % Index as a column to ensure the output is 1-D in the indexed dimension (for now).
Indices.type = '()';
Y = subsref(X, Indices);
% (2) Now do the reshaping part of Gather
shape = size(Y, 1:numDimsX);
if numDimsIdx == 0
    % Delete the indexed dimension
    shape(dltAxis) = [];
elseif numDimsIdx > 1
    % Reshape the indexed dimension into the shape of ONNXIdx
    shape = [shape(1:dltAxis-1) size(ONNXIdx, 1:numDimsIdx) shape(dltAxis+1:end)];
end
% Extend the shape to 2D so it's valid MATLAB
if numel(shape) < 2
    shape = [shape ones(1,2-numel(shape))];
end
Y = reshape(Y, shape);
end

function [Y, numDimsY] = onnxPad(X, pads, value, mode, numDimsX)
% Implements the ONNX Pad operator

% ONNX 'pads' is a vector: [x1_begin, x2_begin...x1_end, x2_end,...], with
% x1,x2, listed in FORWARD ONNX dimension ordering, because it is data
% within a dimension and so is not flipped. xi_begin is the number of
% pixels added at the beginning of axis `i` and xi_end, the number of
% pixels added at the end of axis `i`.  pads can be negative, in which case
% that number of pixels is removed.
numDimsY = numDimsX;
pads = pads(:)';
if numDimsX==1
    % X is Nx1. Temporarily make it reverse-ONNX 2D (1xN), then transpose
    % the result back to 1D at the end.
    X = X';
    numDimsX = 2;
    pads = [pads(1) 0 pads(2) 0];  % Don't pad the dummy dimension
    numDimsY = 1;
end
sizeX  = size(X, 1:numDimsX);
fwdPadMat = reshape(extractdata(pads), [], 2)';  % row1 = begins, row2 = ends
% Columns of padmat are in reverse ONNX ordering. Still the case that row1
% = begins, row2 = ends:
padmat = fliplr(fwdPadMat);
sizeY  = sum([sizeX; padmat]);
% Create output tensor of the right size
Y = value*ones(sizeY, 'like', X);
% Construct subsref indices for inserting (and cropping) the original
for i=1:numel(sizeX)
    Ysubs{i} = max(1,1+padmat(1,i)) : min(sizeY(i), sizeY(i)-padmat(2,i));
    Xsubs{i} = max(1,1-padmat(1,i)) : min(sizeX(i), sizeX(i)+padmat(2,i));
end
Sy      = struct('type', '()');
Sy.subs = Ysubs;
Sx      = struct('type', '()');
Sx.subs = Xsubs;
% Insert/crop the original into the result
Y = subsasgn(Y, Sy, subsref(X, Sx));
% Handle 'reflect' and 'edge' modes, but don't do it if X was 1D, 0x1.
if ismember(mode, ["edge", "reflect"]) && ~(numDimsY==1 && sizeX(2)==0)
    for dim = 1:numDimsX
        if any(padmat(:,dim)>0)
            % Setup a call to subsasgn
            prepad  = padmat(1,dim);
            postpad = padmat(2,dim);
            if prepad > 0
                [Sy, Sx] = prepadIndices(sizeX, prepad, dim, mode);
                Y = subsasgn(Y, Sy, subsref(Y, Sx));
            end
            if postpad > 0
                [Sy, Sx] = postpadIndices(sizeX, sizeY, prepad, postpad, dim, mode);
                Y = subsasgn(Y, Sy, subsref(Y, Sx));
            end
        end
    end
end
% Transpose the result back to 1D if the input was 1D
if numDimsY==1
    Y = Y';
end

% Subfunctions in onnxPad:
    function [Sy, Sx] = prepadIndices(sizeX, prepad, dim, mode)
        Sy   	= struct('type', '()');
        Sy.subs	= repmat({':'}, [1 numel(sizeX)]);
        Sx   	= Sy;
        % Write into the first 'prepad' elements of Y.dim.
        Sy.subs{dim} = 1:prepad;
        switch mode
            case 'reflect'
                % Create indices 2:prepad+1 of X.dim, in the reverse order, with
                % wraparound. Then add prepad to convert them to Y indices.
                Sx.subs{dim} = wrapIndices(prepad+1 : -1 : 2, sizeX(dim)) + prepad;
            case 'edge'
                % Create replicated indices 1 of X.dim. Then add prepad to
                % convert them to Y indices.
                Sx.subs{dim} = repmat(1, [1 prepad]) + prepad;
            otherwise
                assert(false);
        end
    end

    function [Sy, Sx] = postpadIndices(sizeX, sizeY, prepad, postpad, dim, mode)
        Sy   	= struct('type', '()');
        Sy.subs	= repmat({':'}, [1 numel(sizeX)]);
        Sx   	= Sy;
        % Write into the last 'postpad' elements of Y.dim.
        Sy.subs{dim} = sizeY(dim)-postpad+1 : sizeY(dim);
        switch mode
            case 'reflect'
                % Create indices in the reverse order, with wraparound. Then add
                % prepad to convert them to Y indices.
                Sx.subs{dim} = wrapIndices(sizeX(dim)-1 : -1 : sizeX(dim)-postpad, sizeX(dim)) + prepad;
            case 'edge'
                % Create replicated end indices . Then add prepad to convert them
                % to Y indices.
                Sx.subs{dim} = repmat(sizeX(dim), [1 postpad]) + prepad;
            otherwise
                assert(false);
        end
    end

    function j = wrapIndices(i, maxIdx)
        % i can be positive, negative or zero. Legal output indices are in the
        % range 1:maxIdx.
        j = mod(i-1, maxIdx) + 1;
    end
end


function [Y, numDimsY] = onnxShape(X, numDimsX)
% Implements the ONNX Shape operator
% Return the reverse ONNX shape as a 1D column vector
switch numDimsX
    case 0
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(1);
        end
    case 1
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(size(X,1));
        end
    otherwise
        Y = dlarray(fliplr(size(X, 1:numDimsX))');
end
numDimsY = 1;
end

function [poolSize, stride, padding, paddingValue, dataFormat, numDimsY] = prepareAveragePoolArgs(poolSize, stride, padding, count_include_pad, numDimsX)
% Prepares arguments for implementing the ONNX AveragePool operator
poolSize    = fliplr(extractdata(poolSize(:)'));
stride      = fliplr(extractdata(stride(:)'));
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
if isnumeric(padding)
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the fliplr and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
if logical(count_include_pad)
    paddingValue = 0;
else
    paddingValue = 'mean';
end
dataFormat  = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function [offset, scale, datasetMean, datasetVariance, dataFormat, numDimsY, numDimsDatasetMean, numDimsDatasetVariance] = prepareBatchNormalizationArgs(...
    offset, scale, datasetMean, datasetVariance, numDimsX, numDimsDatasetMean, numDimsDatasetVariance)
% Prepares arguments for implementing the ONNX BatchNormalization operator
offset = dlarray(offset,'C');
scale = dlarray(scale,'C');
datasetMean = extractdata(datasetMean);
datasetVariance = extractdata(datasetVariance);
datasetVariance(datasetVariance <= 0) = realmin('single');  % Set nonpositive variance components to a value below eps('single')
dataFormat = [repmat('S', 1, numDimsX-2), 'CB'];
numDimsY = numDimsX;
end

function [weights, bias, stride, dilationFactor, padding, dataFormat, numDimsY] = prepareConvArgs(...
    weights, bias, stride, dilationFactor, padding, numWtGroups, numDimsX, numDimsW)
% Prepares arguments for implementing the ONNX Conv operator

% Weights: The ONNX weight dim is Fcxyz..., where c=C/G, G is numGroups,
% and xyz... are spatial dimensions. DLT "weights" here is the flip of
% that, or ...zyxcF. dlconv requires ...zyxcfG, where f=F/G. So reshape to
% split the last dimension.
sizeW    = size(weights, 1:numDimsW);
F        = sizeW(end);
newWSize = [sizeW(1:numDimsW-1), F/numWtGroups, numWtGroups];
weights  = reshape(weights, newWSize);
% bias
if isempty(bias)
    bias = 0;
end
bias = dlarray(bias(:),'CU');
% Derive missing default attributes from weight tensor
numSpatialDims = numDimsW-2;
if isempty(padding)
    padding = zeros(1, 2*numSpatialDims);
end
if isempty(stride)
    stride = ones(1,numSpatialDims);
end
if isempty(dilationFactor)
    dilationFactor = ones(1,numSpatialDims);
end
% Make the attributes non-dlarrays:
if isa(stride, 'dlarray')
    stride = extractdata(stride);
end
if isa(dilationFactor, 'dlarray')
    dilationFactor = extractdata(dilationFactor);
end
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
% Make the attributes double row vectors, and flip their dimension ordering
% to reverse-onnx:
stride = fliplr(double(stride(:)'));
dilationFactor = fliplr(double(dilationFactor(:)'));
if isnumeric(padding)       % padding can be "same"
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
% Set dataformat and numdims
dataFormat = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function [DLTScales, dataFormat, Method, numDimsY] = prepareResize10Args(ONNXScales, mode, numDimsX)
% Prepares arguments for implementing the ONNX Resize-10 operator

% ONNXScales are in ONNX dimension ordering
DLTScales = flip(extractdata(ONNXScales(:)'));
switch mode
    case "nearest"
        Method = "nearest";
    case "linear"
        Method = "linear";
    otherwise
        assert(false);
end
dataFormat = repmat('S', [1 numDimsX]);
numDimsY = numDimsX;
end

function [S, numDimsY] = prepareSliceArgs(X, Starts, Ends, Axes, Steps, numDimsX)
% Prepares arguments for implementing the ONNX Slice operator

% Starts, Ends and Axes are all origin 0. Axes refer to the ONNX dimension
% ordering, but X uses the reverse, DLT ordering. Starts, Ends, Axes, and
% Steps correspond positionally. Axes and Steps may be omitted, with
% defaults described in the ONNX spec.

% Set default Axes and Steps if not supplied
if isempty(Axes)
    Axes = 0:numDimsX-1;   % All axes
end
Axes(Axes<0) = Axes(Axes<0) + numDimsX; % Handle negative Axes.
if isempty(Steps)
    Steps = ones(1, numel(Starts));
end
% Init all dims to :
S.subs = repmat({':'}, 1, numDimsX);
S.type = '()';
% Set Starts and Ends for each axis
for i = 1:numel(Axes)
    DLTDim = numDimsX - Axes(i);                                               % The DLT dim is the reverse of the ONNX dim.
    % "If a negative value is passed for any of the start or end indices,
    % it represents number of elements before the end of that dimension."
    if Starts(i) < 0
        Starts(i) = size(X,DLTDim) + Starts(i);
    end
    if Ends(i) < 0
        Ends(i) = max(-1, size(X,DLTDim) + Ends(i));                        % The -1 case is when we're slicing backward and want to include 0.
    end
    % "If the value passed to start or end is larger than the n (the number
    % of elements in this dimension), it represents n."
    if Starts(i) > size(X,DLTDim)
        Starts(i) = size(X,DLTDim);
    end
    if Ends(i) > size(X,DLTDim)
        Ends(i) = size(X,DLTDim);
    end
    if Steps(i) > 0
        S.subs{DLTDim} = 1 + (Starts(i) : Steps(i) : Ends(i)-1);            % 1 + (Origin 0 indexing with end index excluded)
    else
        S.subs{DLTDim} = 1 + (Starts(i) : Steps(i) : Ends(i)+1);            % 1 + (Origin 0 indexing with end index excluded)
    end
end
numDimsY = numDimsX;
end

function [perm, numDimsA] = prepareTransposeArgs(ONNXPerm, numDimsA)
% Prepares arguments for implementing the ONNX Transpose operator
if numDimsA <= 1        % Tensors of numDims 0 or 1 are unchanged by ONNX Transpose.
    perm = [];
else
    if isempty(ONNXPerm)        % Empty ONNXPerm means reverse the dimensions.
        perm = numDimsA:-1:1;
    else
        perm = numDimsA-flip(ONNXPerm);
    end
end
end

function [newShape, numDimsY] = prepareUnsqueezeArgs(X, ONNXAxes, numDimsX)
% Prepares arguments for implementing the ONNX Unsqueeze operator
numDimsY = numDimsX + numel(ONNXAxes);
ONNXAxes = extractdata(ONNXAxes);
ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsY;
ONNXAxes = sort(ONNXAxes);                                              % increasing order
if numDimsY == 1
    newShape = size(X);
else
    DLTAxes  = flip(numDimsY - ONNXAxes);                                  % increasing order
    newShape = ones(1, numDimsY);
    posToSet = setdiff(1:numDimsY, DLTAxes, 'stable');
    newShape(posToSet) = size(X, 1:numel(posToSet));
end
end

%% Utility functions:

function s = appendStructs(varargin)
% s = appendStructs(s1, s2,...). Assign all fields in s1, s2,... into s.
if isempty(varargin)
    s = struct;
else
    s = varargin{1};
    for i = 2:numel(varargin)
        fromstr = varargin{i};
        fs = fieldnames(fromstr);
        for j = 1:numel(fs)
            s.(fs{j}) = fromstr.(fs{j});
        end
    end
end
end

function checkInputSize(inputShape, expectedShape, inputName)

if numel(expectedShape)==0
    % The input is a scalar
    if ~isequal(inputShape, [1 1])
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, "[1,1]", inputSizeStr));
    end
elseif numel(expectedShape)==1
    % The input is a vector
    if ~shapeIsColumnVector(inputShape) || ~iSizesMatch({inputShape(1)}, expectedShape)
        expectedShape{2} = 1;
        expectedSizeStr = makeSizeString(expectedShape);
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
else
    % The input has 2 dimensions or more
    
    % The input dimensions have been reversed; flip them back to compare to the
    % expected ONNX shape.
    inputShape = fliplr(inputShape);
    
    % If the expected shape has fewer dims than the input shape, error.
    if numel(expectedShape) < numel(inputShape)
        expectedSizeStr = strjoin(["[", strjoin(string(expectedShape), ","), "]"], "");
        error(message('nnet_cnn_onnx:onnx:InputHasGreaterNDims', inputName, expectedSizeStr));
    end
    
    % Prepad the input shape with trailing ones up to the number of elements in
    % expectedShape
    inputShape = num2cell([ones(1, numel(expectedShape) - length(inputShape)) inputShape]);
    
    % Find the number of variable size dimensions in the expected shape
    numVariableInputs = sum(cellfun(@(x) isa(x, 'char') || isa(x, 'string'), expectedShape));
    
    % Find the number of input dimensions that are not in the expected shape
    % and cannot be represented by a variable dimension
    nonMatchingInputDims = setdiff(string(inputShape), string(expectedShape));
    numNonMatchingInputDims  = numel(nonMatchingInputDims) - numVariableInputs;
    
    expectedSizeStr = makeSizeString(expectedShape);
    inputSizeStr = makeSizeString(inputShape);
    if numNonMatchingInputDims == 0 && ~iSizesMatch(inputShape, expectedShape)
        % The actual and expected input dimensions match, but in
        % a different order. The input needs to be permuted.
        error(message('nnet_cnn_onnx:onnx:InputNeedsPermute',inputName, expectedSizeStr, inputSizeStr));
    elseif numNonMatchingInputDims > 0
        % The actual and expected input sizes do not match.
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
end
end

function doesMatch = iSizesMatch(inputShape, expectedShape)
% Check whether the input and expected shapes match, in order.
% Size elements match if (1) the elements are equal, or (2) the expected
% size element is a variable (represented by a character vector or string)
doesMatch = true;
for i=1:numel(inputShape)
    if ~(isequal(inputShape{i},expectedShape{i}) || ischar(expectedShape{i}) || isstring(expectedShape{i}))
        doesMatch = false;
        return
    end
end
end

function sizeStr = makeSizeString(shape)
sizeStr = strjoin(["[", strjoin(string(shape), ","), "]"], "");
end

function isVec = shapeIsColumnVector(shape)
if numel(shape) == 2 && shape(2) == 1
    isVec = true;
else
    isVec = false;
end
end
function X = makeUnlabeledDlarray(X)
% Make numeric X into an unlabelled dlarray
if isa(X, 'dlarray')
    X = stripdims(X);
elseif isnumeric(X)
    if isinteger(X)
        % Make ints double so they can combine with anything without
        % reducing precision
        X = double(X);
    end
    X = dlarray(X);
end
end

function [Vars, NumDims] = packageVariables(params, inputNames, inputValues, inputNumDims)
% inputNames, inputValues are cell arrays. inputRanks is a numeric vector.
Vars = appendStructs(params.Learnables, params.Nonlearnables, params.State);
NumDims = params.NumDimensions;
% Add graph inputs
for i = 1:numel(inputNames)
    Vars.(inputNames{i}) = inputValues{i};
    NumDims.(inputNames{i}) = inputNumDims(i);
end
end

function X = permuteInputVar(X, userDataPerm, onnxNDims)
% Returns reverse-ONNX ordering
if onnxNDims == 0
    return;
elseif onnxNDims == 1 && isvector(X)
    X = X(:);
    return;
elseif isnumeric(userDataPerm)
    % Permute into reverse ONNX ordering
    if numel(userDataPerm) ~= onnxNDims
        error(message('nnet_cnn_onnx:onnx:InputPermutationSize', numel(userDataPerm), onnxNDims));
    end
    perm = fliplr(userDataPerm);
elseif isequal(userDataPerm, 'auto') && onnxNDims == 4
    % Permute MATLAB HWCN to reverse onnx (WHCN)
    perm = [2 1 3 4];
elseif isequal(userDataPerm, 'as-is')
    % Do not permute the input
    perm = 1:ndims(X);
else
    % userDataPerm is either 'none' or 'auto' with no default, which means
    % it's already in onnx ordering, so just make it reverse onnx
    perm = max(2,onnxNDims):-1:1;
end
X = permute(X, perm);
end

function Y = permuteOutputVar(Y, userDataPerm, onnxNDims)
switch onnxNDims
    case 0
        perm = [];
    case 1
        if isnumeric(userDataPerm)
            % Use the user's permutation because Y is a column vector which
            % already matches ONNX.
            perm = userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            % Treat the 1D onnx vector as a 2D column and transpose it
            perm = [2 1];
        else
            % userDataPerm is 'none'. Leave Y alone because it already
            % matches onnx.
            perm = [];
        end
    otherwise
        % ndims >= 2
        if isnumeric(userDataPerm)
            % Use the inverse of the user's permutation. This is not just the
            % flip of the permutation vector.
            perm = onnxNDims + 1 - userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            if onnxNDims == 2
                % Permute reverse ONNX CN to DLT CN (do nothing)
                perm = [];
            elseif onnxNDims == 4
                % Permute reverse onnx (WHCN) to MATLAB HWCN
                perm = [2 1 3 4];
            else
                % User wants the output in ONNX ordering, so just reverse it from
                % reverse onnx
                perm = onnxNDims:-1:1;
            end
        elseif isequal(userDataPerm, 'as-is')
            % Do not permute the input
            perm = 1:ndims(Y);
        else
            % userDataPerm is 'none', so just make it reverse onnx
            perm = onnxNDims:-1:1;
        end
end
if ~isempty(perm)
    Y = permute(Y, perm);
end
end

function s = updateStruct(s, t)
% Set all existing fields in s from fields in t, ignoring extra fields in t.
for name = transpose(fieldnames(s))
    s.(name{1}) = t.(name{1});
end
end
